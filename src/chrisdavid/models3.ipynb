{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to explore Random Forest Regressors and Support Vector Machines along with some input variables to see if we can make a model that accurately forecasts energy demand. We aim to achieve better results than simply saying 'The demand in 30 minutes time will be the same as it is right now'. This is calculated below as having a mean average loss of 218."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Suppress annoying deprecation warnings\n",
    "from warnings import filterwarnings  # noqa\n",
    "filterwarnings(action='ignore',\n",
    "                        category=DeprecationWarning,\n",
    "                        module='sklearn')  # noqa\n",
    "\n",
    "# Import libraries\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import make_scorer\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.svm import SVR\n",
    "#import forestci as fci\n",
    "\n",
    "## Set path to current directory\n",
    "#sys.path.append(os.path.abspath(os.getcwd()))\n",
    "\n",
    "## read data\n",
    "data = pd.read_csv('F:/Users/Dave/Desktop/UNSW-Capstone-Group-E/src/chrisdavid/Cleaned_Data_mkII.csv')\n",
    "\n",
    "# Set random state\n",
    "STATE = 69\n",
    "\n",
    "# If set to true, models will perform hyperparameter tuning. If false they will use\n",
    "# pre-defined values\n",
    "SEARCH = False\n",
    "\n",
    "# Makes sure datetime is in datetime format\n",
    "data['DATETIME'] = pd.to_datetime(data['DATETIME'])\n",
    "\n",
    "# We want to test a time column as an input\n",
    "data['time'] = (data['DATETIME'].dt.strftime(\"%H%M%S\"))\n",
    "\n",
    "# Also would like to test demand and tmeperature 30, 60 and 90 mins before the current time as input\n",
    "# These were found to have diminishing return after 60 mins out for demand and 30 mins out for temp\n",
    "data['demand_30'] = data.TOTALDEMAND.shift(1)\n",
    "data['demand_60'] = data.TOTALDEMAND.shift(2)\n",
    "data['demand_90'] = data.TOTALDEMAND.shift(3)\n",
    "\n",
    "data['temp_30'] = data.TEMPERATURE.shift(1)\n",
    "data['temp_60'] = data.TEMPERATURE.shift(2)\n",
    "data['temp_90'] = data.TEMPERATURE.shift(3)\n",
    "\n",
    "######### Change the number of years\n",
    "# Select only data from the past 1 years to make hyperparameter tuning take less time\n",
    "mask = (data['DATETIME'] >= '2020-01-01') & (data['DATETIME'] < '2021-01-01')\n",
    "data = data.loc[mask]\n",
    "\n",
    "## How accurate would it be to use demand 30 mins ago as a prediciton for demand now?\n",
    "#round(math.sqrt(mean_squared_error(data['TOTALDEMAND'], data['demand_30'])))\n",
    "\n",
    "\n",
    "# And an indictor if it is a weekend or not\n",
    "data['is_weekday'] = data['DATETIME'].dt.weekday\n",
    "data['is_weekday'] = np.where(data['is_weekday'] < 5, 1, 0)\n",
    "\n",
    "# import weather data\n",
    "weather = pd.read_csv('F:/Users/Dave/Desktop/UNSW-Capstone-Group-E/src/chrisdavid/weather.csv')\n",
    "\n",
    "# Convert datetime to date\n",
    "weather['date'] = pd.to_datetime(weather[['Year','Month','Day']]).dt.date\n",
    "\n",
    "# Extract just day and rain columns\n",
    "weather.columns = ['drop1','drop2','drop3','drop4','drop5','rain','drop6','drop7','date']\n",
    "weather = weather[['date','rain']]\n",
    "\n",
    "# Fill NA values\n",
    "weather['rain'].fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "## Create dummy variables for rain for better model use\n",
    "#weather['RAIN'] = pd.cut(weather['rain'],bins=[-1,0.2,4,10,999999],labels=['NONE','LIGHT','MODERATE','HEAVY'])\n",
    "#weather_dummy = pd.get_dummies(weather,columns=['RAIN'])\n",
    "\n",
    "# Create day column in data and merge using it\n",
    "data['date'] = data['DATETIME'].dt.date\n",
    "\n",
    "data = data.merge(weather, on = 'date')\n",
    "\n",
    "data['day'] = data['DATETIME'].dt.day\n",
    "data['month'] = data['DATETIME'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a data frame with all of the features that we deem potentially interesting, we would like to evaluate their suitability for use in a random forest regressor. To do feature selection we will use mutual information (information gain of each input in relation to the output variable) as a selection metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code takes ages to run\n",
    "#filterwarnings(action='ignore',\n",
    "#                        category=DeprecationWarning,\n",
    "#                        module='sklearn')  # noqa\n",
    "#\n",
    "#from sklearn.feature_selection import mutual_info_regression  \n",
    "#from sklearn.feature_selection import SelectKBest\n",
    "#\n",
    "#Y = data['TOTALDEMAND']\n",
    "#X = data.drop(columns = ['TEMPERATURE', 'TOTALDEMAND', 'DATETIME', 'date'])\n",
    "#\n",
    "#selector = SelectKBest(mutual_info_regression, k = 'all')\n",
    "#\n",
    "#X_train_new = selector.fit_transform(X, Y)  \n",
    "#mask = selector.get_support()\n",
    "#\n",
    "#new_features = X.columns[mask]\n",
    "#\n",
    "#for i in range(len(new_features)):\n",
    "#    print('Score: ', selector.scores_[i], 'Feature: ', new_features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that time, demand_30, demand_60, demand_90, temp_30, temp_60, temp_90 and month are all potentially useful in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Random Forest MAE:  57.15502132366357\n",
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'mae',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 10,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 69,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Create features and target sets\n",
    "base_features = data[['time', 'demand_30', 'demand_60', 'demand_90', 'temp_30', 'temp_60' ,'temp_90', 'month']]\n",
    "base_target = data['TOTALDEMAND']\n",
    "\n",
    "# Convert to numpy arrays and split training/test data\n",
    "base_features_np = pd.DataFrame(base_features).to_numpy()\n",
    "base_target_np = np.ravel(pd.DataFrame(base_target).to_numpy())\n",
    "\n",
    "base_features_train, base_features_test, base_target_train, base_target_test = train_test_split(base_features_np,\n",
    "                                                                            base_target_np, random_state = STATE)\n",
    "\n",
    "# Implement Random Forest\n",
    "base_rnd_clf = RandomForestRegressor(random_state = STATE, criterion = 'mae')\n",
    "base_rnd_clf.fit(base_features_train, base_target_train)\n",
    "\n",
    "# Print error\n",
    "base_rf_predicted = base_rnd_clf.predict(base_features_test)\n",
    "base_rf_MAE = mean_absolute_error(base_target_test, base_rf_predicted)\n",
    "#base_rf_error = round(math.sqrt(mean_squared_error(base_target_test, base_rf_predicted)))\n",
    "print(\"Baseline Random Forest MAE: \", base_rf_MAE)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(base_rnd_clf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually quite a good model performance. We suspect that there is a very high correlation between the demand and temperature variables, so we will try eliminating some of these with the suspicion that dropping them will not drastically effect the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and target sets\n",
    "features = data[['time', 'demand_30', 'demand_60', 'temp_30']]\n",
    "target = data['TOTALDEMAND']\n",
    "\n",
    "# Convert to numpy arrays and split training/test data\n",
    "features_np = pd.DataFrame(features).to_numpy()\n",
    "target_np = np.ravel(pd.DataFrame(target).to_numpy())\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_np,\n",
    "                                                                            target_np, random_state = STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Random Forest MAE:  59.925383164568494\n",
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'mae',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 10,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 69,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Implement Random Forest\n",
    "rnd_clf = RandomForestRegressor(random_state = STATE, criterion = 'mae')\n",
    "rnd_clf.fit(features_train, target_train)\n",
    "\n",
    "# Print error\n",
    "rf_predicted = rnd_clf.predict(features_test)\n",
    "rf_MAE = mean_absolute_error(target_test, rf_predicted)\n",
    "print(\"Baseline Random Forest MAE: \", rf_MAE)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rnd_clf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have managed to eliminate most of the remaining inputs. We have; time, demand_30, demand_60 and temp_30 as useful to our model.\n",
    "\n",
    "Let's try random search of optimizing hyperparamters as a quick way to tell if our baseline model can perform any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 34.6min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 64.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Random Forest MAE:  57.95956222736393\n",
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'mae',\n",
      " 'max_depth': 18,\n",
      " 'max_features': 'auto',\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 6,\n",
      " 'n_estimators': 22}\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': list(range(12, 24, 2)),\n",
    "               'criterion': ['mae'],\n",
    "               'max_features': ['auto'],\n",
    "               'max_depth': list(range(1, 20, 1)),\n",
    "               'min_samples_split': list(range(5, 9, 1)),\n",
    "               'min_samples_leaf': list(range(2, 10, 1)),\n",
    "               'bootstrap': [True]}\n",
    "\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = RandomForestRegressor(), param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 5, verbose = 2, random_state = STATE, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(features_train, target_train)\n",
    "\n",
    "## Print error\n",
    "rf_random_pred = rf_random.best_estimator_.predict(features_test)\n",
    "rf_random_MAE = mean_absolute_error(target_test, rf_random_pred)\n",
    "print(\"Baseline Random Forest MAE: \", rf_random_MAE)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tuning is only a little better than our base model. No point in continuing hyperparpmeter tuning further, or in continuing with the model, unless we can think of other inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now start with SVM modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Support Vector Machine Regressor\n",
    "\n",
    "\n",
    "regressor = SVR(kernel = 'linear')\n",
    "regressor.fit(features_train, target_train)\n",
    "\n",
    "# Print error\n",
    "svm_predicted = regressor.predict(features_test)\n",
    "svm_MAE = mean_absolute_error(target_test, svm_predicted)\n",
    "print(\"SVM MAE: \", svm_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "SVM_param_grid = {'C' : [0.1, 1, 10, 100, 1000],\n",
    "                 'gamma' : [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                 'kernel' : ['poly', 'rbf']}\n",
    "\n",
    "scorer = make_scorer(mean_absolute_error, greater_is_better = False)\n",
    "\n",
    "svr_gs = GridSearchCV(SVR(), SVM_param_grid, scoring=scorer)\n",
    "svr_gs.fit(features_train, target_train)\n",
    "\n",
    "grid_preds = svr_gs.predict(features_test)\n",
    "best_SVM_grid_error = round(math.sqrt(mean_squared_error(base_target_test, grid_preds)))\n",
    "\n",
    "# Print error\n",
    "#print(\"SVM Grid Search Tuning Error: \", best_SVM_grid_error)\n",
    "\n",
    "# Look at parameters used\n",
    "#print('Parameters currently in use:\\n')\n",
    "#pprint(svr_gs.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate RMSE for each model and benchmark\n",
    "# Lower = better model\n",
    "\n",
    "rf_error = round(math.sqrt(mean_squared_error(target_test, rf_predicted)))\n",
    "svm_error = round(math.sqrt(mean_squared_error(target_test, svm_predicted)))\n",
    "benchmark_error = round(math.sqrt(mean_squared_error(data['TOTALDEMAND'], data['demand_30'])))\n",
    "\n",
    "print(\"FINAL SCORES\")\n",
    "print(\"Untuned Random Forest Error: \", rf_error)\n",
    "print(\"Tuned Random Forest Error: \", best_random_error)\n",
    "print(\"Untuned SVM Error: \", svm_error)\n",
    "print(\"Tuned SVM Error: \", best_SVM_grid_error)\n",
    "print(\"Benchmark Error: \", benchmark_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "predictions = list(rf_random.predict(base_features_np))\n",
    "targets = list(base_target_np)\n",
    "output = pd.DataFrame({'prediction':predictions,'target':targets})\n",
    "output.to_csv('output_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
