{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f0c407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lossFunc assigned\n",
      "starting\n",
      "test\n",
      "Using device: cpu\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'lossFunc' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ae240a1c3e2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'starting'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-ae240a1c3e2a>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[1;31m# Get model and optimiser from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# net = net.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mlossFunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[0moptimiser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'lossFunc' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "py\n",
    "\n",
    "Group E Team - UNSW Capstone Project\n",
    "\n",
    "################################################################################\n",
    "#            UNSW CAPSTONE PROJECT                                             #\n",
    "#            GROUP - E Team Members                                            #\n",
    "#            Last Updated Date:  30 Mar 2022                                   #\n",
    "#            Description: This program is used to define different types of NN #\n",
    "#                         The hyperparameters of the Neural Networks are also  #\n",
    "#                         defined in this section. There are 4 different types #\n",
    "#                         of Neural Networks are defined here. The Architecture#\n",
    "#                         of these 4 Neural Networks are different. Thus       #\n",
    "#                         a global parameter is used to control which Architec-#\n",
    "#                         ture is used when running the program.               # \n",
    "################################################################################ \n",
    "\n",
    "Energy Demand Forecasting using Neural Networks and Deep Learning\n",
    "\n",
    "This file is used to create additional variables, functions, classes, etc.\n",
    "and this code runs with the main program Forecast_Demand_Main.py file \n",
    "\n",
    "The default valaues of  trainValSplit, batchSize, epochs, and optimiser, can \n",
    "be found on Forecast_Demand_Main.  We can further modify these modify these \n",
    "to improve the performance our model. This will help us to predict \n",
    "Energy Demand \n",
    "\n",
    "The variable device may be used to refer to the CPU/GPU being used by PyTorch.\n",
    "You may change this variable in the config.py file.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torch.nn.functional as tnnfunc\n",
    "import torch.optim as toptim\n",
    "import math\n",
    "from config import device\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------Global Variable ---------------------------------#\n",
    "#                 Other global variables used in py              # \n",
    "#-------------------------------------------- ---------------------------------#\n",
    "\n",
    "\n",
    "input_dim = 4  \n",
    "# This is the input dimension to the neural network.\n",
    "# At present there will be 4 dimention \n",
    "# Temperature, Day, Month, and Time Interval\n",
    "\n",
    "# Different Architecture are defined below to test with different \n",
    "# Neural Network\n",
    "\n",
    "architecture = 1 # Can have values 1, 2, 3, 4\n",
    "# This Architecture Value can be changed in order to change the \n",
    "# Neural Network Architecture. Below are the details of the value\n",
    "# 1 represents LSTM Network + 1 fully connected linear layer + Output Layer\n",
    "# 2 represents LSTM Network + 2 Fully Connected linear Layer + Output Layer\n",
    "# 3 represents GRU Network + 1 Fully Connected linear Layer + Output Layer\n",
    "# 4 represents GRU Network + 2 Fully Connected linear Layer + Output Layer\n",
    "\n",
    "optimiser_choice = 2 # Can have  1 or 2\n",
    "# The options for optimiser is as mentioned below\n",
    "# 1  -  SGD Optimiser\n",
    "# 2  -  ADAM Optimiser\n",
    "\n",
    "LSTM_GRU_Hidden_Size = 16\n",
    "# This is the output Size of either the LSTM or GRU Network based on the \n",
    "# architecture chosen\n",
    "\n",
    "LSTM_GRU_Num_Layers = 1\n",
    "# This is the number of hidden layers in either the LSTM or GRU Network \n",
    "# chosen as per the architecture.\n",
    "\n",
    "Linear_Layer1_output_size = 8\n",
    "# This is the output size of the first linear Layer which is connected to\n",
    "# either GNU or LSTM as per the architecture choices made above.\n",
    "\n",
    "Linear_Layer2_output_size = 120\n",
    "# This is the output size of the second linear Layer which is connected to\n",
    "# either GNU or LSTM as per the architecture choices made above.\n",
    "\n",
    "nn_output_size = 1\n",
    "# This is the output size of neural network.\n",
    "# The model output will have predictions it makes in\n",
    "# the same format as the energy demand dataset.  \n",
    "# The predictions must be of type float\n",
    "# so there will be one output which will forecast\n",
    "# what the energy demand will be.\n",
    "\n",
    "learning_rate = 0.01 #0.032\n",
    "# this is the learning rate used in the optimizers\n",
    "\n",
    "weight_decay = 0.0001\n",
    "# weight decay used in ADAM Optimiser\n",
    "\n",
    "training_dataset_filename= 'training_dataset.csv'\n",
    "#Training dataset path\n",
    "\n",
    "trainValSplit = 1\n",
    "# Training and Validation Data set Split\n",
    "# Need to work on the way to split tensordataset\n",
    "# in training and Validation Set\n",
    "\n",
    "batchSize = 1000\n",
    "# setting the batch size as a global parameter.\n",
    "\n",
    "epochs = 10 \n",
    "\n",
    "\n",
    "################################################################################\n",
    "##### The following determines the loads and processes the data           ######\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def upload_data():\n",
    "    \"\"\"\n",
    "    This function is used to load the data\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    training_dataset = pd.read_csv(training_dataset_filename) \n",
    "    return training_dataset\n",
    "\n",
    "def process_data(training_dataset):\n",
    "    \"\"\"\n",
    "    The data is processed and converted in tensors\n",
    "    \"\"\"\n",
    "    training_dataset['TEMPERATURE'] = training_dataset['TEMPERATURE']\n",
    "\n",
    "    training_dataset['Date_object'] = training_dataset['Date'].apply(pd.to_datetime, format='%d/%m/%Y')\n",
    "\n",
    "    training_dataset['month'] = training_dataset['Date_object'].apply(lambda x: x.month)\n",
    "\n",
    "    training_dataset['day'] = training_dataset['Date_object'].apply(lambda x: x.day)\n",
    "\n",
    "    times = list(training_dataset['Time'][:48])\n",
    "\n",
    "    training_dataset['time_int'] = training_dataset['Time'].apply(lambda x: times.index(x))\n",
    "\n",
    "    X = training_dataset[['TEMPERATURE', 'day', 'month', 'time_int']]\n",
    "\n",
    "    x = torch.tensor(X.values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # creating tensor from targets_df \n",
    "    Y = training_dataset[['TOTALDEMAND']]\n",
    "    y = torch.tensor(Y.values)\n",
    "\n",
    "    # torch can only train on Variable, so convert them to Variable\n",
    "    x_data, y_data = Variable(x), Variable(y)\n",
    "\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "################################################################################\n",
    "###################### The following determines the model ######################\n",
    "################################################################################\n",
    "\n",
    "class network(tnn.Module):\n",
    "    \"\"\"\n",
    "    Class for creating the neural network.  The input to this network will be a\n",
    "    tensor which has the temperature the day month and time of the year. The forward method\n",
    "    should return an output for the energy demand.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(network, self).__init__()\n",
    "\n",
    "\n",
    "  # The neural network will take the temperature vector as an input.\n",
    "  # The temperature Vector will have Temperature, Day, Month, Time_Int\n",
    "  # Thus it is necessary that neural network input layer dimension \n",
    "  # remains same as the time vector dimension which is 4.\n",
    "        # Define the Neural Network Parameters below\n",
    "        self.input_size = input_dim \n",
    "        self.hidden_size = LSTM_GRU_Hidden_Size\n",
    "        self.num_layers = LSTM_GRU_Num_Layers\n",
    "        self.linear_output_size1 = Linear_Layer1_output_size\n",
    "        self.linear_output_size2 = Linear_Layer2_output_size\n",
    "        self.tnn_output = nn_output_size\n",
    "        self.tnn_input = None # This will be determined based on architecture\n",
    "\n",
    "        # the program checks the architecture and then builds the network based on the \n",
    "        # architecture values. When the architecture value is equal to 1 or 2 the Recurring Neural \n",
    "        # Network (RNN) is created using LSTM and when the architecture value is 3 or 4 the RNN\n",
    "        # network is created using GRU. \n",
    "        if architecture == 1 or architecture == 2 or architecture == 3 or architecture == 4:\n",
    "            # if proper architecture is choosen then the network is built accordingly\n",
    "            # change the architecture value to build a specific network\n",
    "            if architecture == 1 or architecture == 2:\n",
    "                #------------------------------LSTM NETWORK -------------------------------# \n",
    "                # Generating an LSTM Network using the parameters set above.\n",
    "                # this LSTM Network takes the input size same as the word vector\n",
    "                # dimension\n",
    "                self.lstm = tnn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size,\n",
    "                                  num_layers=self.num_layers, batch_first=True)\n",
    "                #--------------------------------------------------------------------------# \n",
    "            else:\n",
    "                # this is the case for architecture == 3 or architecture == 4\n",
    "                #------------------------------GRU NETWORK -------------------------------# \n",
    "                # Generating a GRU Network using the parameters set above.\n",
    "                # this GRU Network takes the input size same as the word vector\n",
    "                # dimension\n",
    "                self.gru = tnn.GRU(input_size=self.input_size, hidden_size=self.hidden_size,\n",
    "                                  num_layers=self.num_layers, batch_first=True)\n",
    "                #--------------------------------------------------------------------------# \n",
    "\n",
    "            # the output from the LSTM / GRU network is passed to a fully connected\n",
    "            # Linear network. So the input to this linear network is same as the \n",
    "            # hidden size of the  Network and the output is defined by the parameter \n",
    "            # linear_output_size1\n",
    "            self.fully_connected_layer1 =  tnn.Linear(self.hidden_size, self.linear_output_size1)\n",
    "\n",
    "            if architecture == 1 or architecture == 3:\n",
    "                # In this case the output of the first fully connected layer is fed\n",
    "                # to the input of the output layer\n",
    "                self.tnn_input = self.linear_output_size1\n",
    "            else:\n",
    "                # In this case the output of the first fully connected layer is fed\n",
    "                # to the input of the second fully connected layer\n",
    "                self.fully_connected_layer2 =  tnn.Linear(self.linear_output_size1, self.linear_output_size2 )\n",
    "\n",
    "                # Further the output of the second fully connected layer is fed\n",
    "                # to the output layer\n",
    "                self.tnn_input = self.linear_output_size2\n",
    "\n",
    "\n",
    "            # The output of the last fully connected \n",
    "            # Linear network is fed into the input of the output layer\n",
    "            # and the output from the output layer is defined by \n",
    "            # the parameter tnn_output\n",
    "            self.output =  tnn.Linear(self.tnn_input , self.tnn_output)\n",
    "\n",
    "            # Initializing Weight Buffer for fully connected layer\n",
    "            # which will be activated lated using RELU Activation.\n",
    "            torch.nn.init.kaiming_normal_(self.fully_connected_layer1.weight.data)\n",
    "\n",
    "            # finding the standard deviation using the input size of \n",
    "            # of the output layer. This will be used to initialise\n",
    "            # output layer weights\n",
    "            stdv = 1.0 / math.sqrt(self.tnn_input)      \n",
    "            self.output.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "            # initialise the bias of the two layers\n",
    "            self.fully_connected_layer1.bias.data.zero_()\n",
    "            self.output.bias.data.zero_()\n",
    "\n",
    "            if architecture == 2 or architecture == 4:\n",
    "                #Initialise the weights of second linear network.\n",
    "                # which will be activated lated using RELU Activation.\n",
    "                torch.nn.init.kaiming_normal_(self.fully_connected_layer2.weight.data)\n",
    "\n",
    "                #Initialise the bias of second linear network.\n",
    "                # which will be activated lated using RELU Activation.\n",
    "                self.fully_connected_layer2.bias.data.zero_()                         \n",
    "\n",
    "        else:\n",
    "            # this is the case when wrong architecture option is added \n",
    "            assert False, 'Wrong Value of Architecture assigned during Global Variable Declaration'\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # passing the input to the embedding layer.\n",
    "\n",
    "\n",
    "        if architecture == 1 or architecture == 2:\n",
    "            # This is the scenario for LSTM Network\n",
    "            #------------------------------LSTM Forward Propogation---------------------# \n",
    "            # hidden state\n",
    "            h_0 = torch.zeros(self.num_layers, self.hidden_size).to(device) \n",
    "\n",
    "            # internal state\n",
    "            c_0 = torch.zeros(self.num_layers, self.hidden_size).to(device) \n",
    "\n",
    "            # Initiate weights before tanh activation\n",
    "            torch.nn.init.xavier_normal_(h_0) \n",
    "            torch.nn.init.xavier_normal_(c_0) \n",
    "\n",
    "            # Propagate input through LSTM network\n",
    "            output, (h_n, c_n) = self.lstm(input, (h_0, c_0)) \n",
    "            #lstm with input, hidden, and internal state\n",
    "\n",
    "            #--------------------------------------------------------------------------#\n",
    "\n",
    "        elif architecture == 3 or architecture == 4:\n",
    "            # This is the scenario for GRU Network\n",
    "            #------------------------------GRU Forward Propogation---------------------# \n",
    "            # hidden state\n",
    "            h_0 = torch.zeros(self.num_layers, self.hidden_size).to(device) \n",
    "            # Initiate weights before tanh activation\n",
    "            torch.nn.init.xavier_normal_(h_0) \n",
    "            # Propagate input through GRU network\n",
    "            output, (h_n) = self.gru(input, (h_0)) \n",
    "            #-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "        # reshaping the data and passing it to the \n",
    "        # fully connected dense layer \n",
    "        \n",
    "        out = self.fully_connected_layer1(output) \n",
    "\n",
    "        # applying relu activation to the output \n",
    "        # from the fully connected dense layer\n",
    "        out = torch.relu(out)\n",
    "\n",
    "        if architecture == 2 or architecture == 4:\n",
    "            # This is the case where RNN Network is connected\n",
    "            # to two fully connected linear dense network\n",
    "            # reshaping the data and passing it to the \n",
    "            # ----second fully connected dense layer ----------------------------------#\n",
    "            out = self.fully_connected_layer2(out) \n",
    "\n",
    "            # applying relu activation to the output \n",
    "            # from the second fully connected dense layer\n",
    "            out = torch.relu(out)\n",
    "            # -------------------------------------------------------------------------#\n",
    "\n",
    "        # passing the activated output to the final \n",
    "        # output\n",
    "        out = self.output(out) \n",
    "\n",
    "\n",
    "        # The final output is activated using\n",
    "        #  Softmax. Thus using the log\n",
    "        # softmax activation with dimension 1\n",
    "        DemandOutput = torch.relu(out)\n",
    "\n",
    "        return DemandOutput\n",
    "\n",
    "\n",
    "class loss(tnn.Module):\n",
    "    \"\"\"\n",
    "    Class for creating the loss function.  The actual demand (DemandTarget)\n",
    "    and predicted demand output from the network (DemandOutput) \n",
    "    will be passed to the forward method during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(loss, self).__init__()\n",
    "        self.loss_demand = tnnfunc.l1_loss\n",
    "\n",
    "\n",
    "    def forward(self, DemandOutput, DemandTarget):\n",
    "        DemandTarget = DemandTarget.to(torch.float).to(device)\n",
    "        demand_loss = self.loss_demand(DemandOutput, DemandTarget)\n",
    "        return demand_loss\n",
    "\n",
    "net = network()\n",
    "lossFunc = loss()\n",
    "print('lossFunc assigned')\n",
    "\n",
    "################################################################################\n",
    "################## The following determines training options ###################\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "if optimiser_choice==1 :\n",
    "    # for choice 1 the SGD Optimiser is called\n",
    "    optimiser = toptim.SGD(net.parameters(), lr=learning_rate)\n",
    "elif optimiser_choice==2 :\n",
    "    # The choice of Option 2 uses ADAM Optimiser\n",
    "    optimiser = toptim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "else :\n",
    "    # Wrong value of optimiser_choice assigned.\n",
    "    assert False, 'Wrong value of optimiser_choice assigned.'\n",
    "    \n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Group E Team - UNSW Capstone Project\n",
    "################################################################################\n",
    "#            UNSW CAPSTONE PROJECT                                             #\n",
    "#            GROUP - E Team Members                                            #\n",
    "#            Last Updated Date:  30 Mar 2022                                   #\n",
    "#            Description: This is the main file  which created  the            #\n",
    "#                         training dataset and splits the training dataset     #\n",
    "#                         in different batches                                 # \n",
    "#                         This further saves the trained model which then      #\n",
    "#                         can be used to predict demand based on temperature   #   \n",
    "#                         This program also runs the validation tests and      #\n",
    "#                         prints the output of the test results                #     \n",
    "#                                                                              #\n",
    "################################################################################\n",
    "Forecast_Demand_Main\n",
    "This is the main program file\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Commenting the below line because\n",
    "# module 'torchtext.data' has no attribute \n",
    "# 'Field'\n",
    "# from torchtext import data\n",
    "\n",
    "# from torchtext.legacy import data\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from config import device\n",
    "#import energy_demand\n",
    "\n",
    "def main():\n",
    "    #lossFunc = loss()\n",
    "    print('test')\n",
    "    print(\"Using device: {}\"\n",
    "          \"\\n\".format(str(device)))\n",
    "\n",
    "    # Load the training dataset\n",
    "    training_dataset = upload_data()\n",
    "\n",
    "    # process the data and store in tensor\n",
    "    x, y = process_data(training_dataset)\n",
    "\n",
    "    # prepare the dataset using the tensor data\n",
    "    dataset = Data.TensorDataset(x, y)\n",
    "\n",
    "    # Allow training on the entire dataset, or split it for training and validation.\n",
    "    if trainValSplit == 1:\n",
    "        trainLoader = Data.DataLoader( \n",
    "                                dataset= dataset,\n",
    "                                batch_size=batchSize, \n",
    "                                shuffle=True, num_workers=1)\n",
    "    else:\n",
    "        train_len = round(trainValSplit*len(dataset))\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_len, len(dataset) - train_len])\n",
    "        # need to handle the split functionality\n",
    "        trainLoader = Data.DataLoader( \n",
    "                                dataset= train_dataset,\n",
    "                                batch_size=batchSize, \n",
    "                                shuffle=True, num_workers=1)\n",
    "\n",
    "        testLoader = Data.DataLoader( \n",
    "                                dataset= test_dataset,\n",
    "                                batch_size=batchSize, \n",
    "                                shuffle=True, num_workers=1)\n",
    "\n",
    "    # Get model and optimiser from \n",
    "    # net = net.to(device)\n",
    "    lossFunc = lossFunc\n",
    "    optimiser = optimiser\n",
    "\n",
    "    # Train.\n",
    "    for epoch in range(epochs):\n",
    "        runningLoss = 0\n",
    "\n",
    "        for i, batch in enumerate(trainLoader):\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs , DemandTarget = batch\n",
    "\n",
    "\n",
    "            # converting the inputs to float\n",
    "            inputs, DemandTarget = inputs.float(), DemandTarget.float()\n",
    "\n",
    "\n",
    "            # PyTorch calculates gradients by accumulating contributions to them\n",
    "            # (useful for RNNs).  Hence we must manually set them to zero before\n",
    "            # calculating them.\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Forward pass through the network.\n",
    "            DemandOutput = net(inputs)\n",
    "            \n",
    "            loss = lossFunc(DemandOutput.view(-1), DemandTarget.view(-1))\n",
    "\n",
    "            # Calculate gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Minimise the loss according to the gradient.\n",
    "            optimiser.step()\n",
    "\n",
    "            runningLoss += loss.item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\"\n",
    "                      % (epoch + 1, i + 1, runningLoss / 10))\n",
    "                runningLoss = 0\n",
    "\n",
    "\n",
    "    if trainValSplit != 1:\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0.0\n",
    "            for batch in testLoader:\n",
    "                inputs , DemandTarget = batch\n",
    "\n",
    "                inputs, DemandTarget = inputs.float(), DemandTarget.float()\n",
    "\n",
    "                DemandOutput = net(inputs)\n",
    "\n",
    "                loss = lossFunc(DemandOutput.view(-1), DemandTarget.view(-1))\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "            print(\"Validation Loss: %.3f\" % (valid_loss/len(testLoader)))\n",
    "\n",
    "print('starting')\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2186814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
