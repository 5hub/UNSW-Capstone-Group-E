---
title: "Cleaning the forecast_nsw dataset"
authors: "David Anderson z5343521"
date: "19/03/2022"
output: html_document
---

Now that the forecast NSW dataset has been reduced, I want to see what information can be extracted from it.

```{r}
# Import libraries
library(lubridate)
library(dplyr)
library(data.table)

```

```{r}
## Import forecast dataset      
#setwd('../')
#getwd()
#fcst_nsw <- read.csv(unz("David/forecastdemand_nsw_reduced.zip", "forecastdemand_nsw_reduced.csv"), header = T)

# Import forecast dataset
fcst_nsw = fread("forecastdemand_nsw_reduced.csv.gz")
```

```{r}
# Create a new dataframe so that I can work on it and can re-load it from the original data without having to read the zipped file again which takes ages.
fcst_nsw2 <- fcst_nsw

# Convert the datetimes using lubridate
fcst_nsw2$LASTCHANGED <- ymd_hms(fcst_nsw2$LASTCHANGED)
fcst_nsw2$DATETIME <- ymd_hms(fcst_nsw2$DATETIME)

# Check to see how many forecasts there are from each model.
as.data.frame(table(fcst_nsw2$PERIODID))
```

It looks like there are forecasts for all 196505 30 minute intervals of demand. The huge size of this dataset is in the fact that there are 79 different sets of forecasts, one for each 30 min interval before the actual datetime (from 30 mins to 79*30mins = 39hrs 30mins before the actual DATETIME).
There are a fair few missing values after PERIODID 32. For the sake of brevity at the moment I will only include 32 forecast models.

#################This might need to change later to include all of them. This would likely involve filling in missing values however and would take time. ###########

```{r}
## Calculate the difference between forecast and actual energy demand
#fcst_nsw2 <- fcst_nsw2 %>%
#  mutate(Time_since_Forecast = (DATETIME - LASTCHANGED))

# Make 32 different dataframes, each containing the data from forecast 1*30mins out to 32*30mins out
for (i in c(1,32)){
  assign(paste0("Fcst", i), fcst_nsw2[fcst_nsw2$PERIODID == i, ][, -c(1, 3)])
  
}

#Import cleaned dataset
NSW_data <- read.csv("../../report/Cleaned_Data.csv")

# Convert date and time to a datetime
NSW_data$DATETIME <- paste(NSW_data$Date, NSW_data$Time)
NSW_data$DATETIME <- dmy_hm(NSW_data$DATETIME)
NSW_data <- NSW_data[, -c(2,3)]

# Rename the forecast demand columns to something more appropriate for each forecast
Fcst1 <- rename(Fcst1, Fcst1 = FORECASTDEMAND)
Fcst32 <- rename(Fcst32, Fcst32 = FORECASTDEMAND)

# Do a left merge on NSW_data and the forecast1 and forecast31 data
NSW_data <- left_join(NSW_data, Fcst1, by = 'DATETIME', copy = TRUE)
NSW_data <- left_join(NSW_data, Fcst32, by = 'DATETIME', copy = TRUE)

# Create a column containing how much this forecast was away from the actual consumption
NSW_data$Diff1 <- NSW_data$TOTALDEMAND - NSW_data$Fcst1
NSW_data$Diff32 <- NSW_data$TOTALDEMAND - NSW_data$Fcst32

# Plot this difference
boxplot(NSW_data$Diff1, NSW_data$Diff32)

# Caluculate five number summaries
fivenum(NSW_data$Diff1)
fivenum(NSW_data$Diff32)
```

I think these 5 number summaries are a good metric to evaluate any future models that we come up with.
















Some code that doesn't work yet. This is how I think I need to simplify some of the above code.

```{r}
for (i in 1:32){
  assign(paste0("Fcst", i)[1], toString(paste0("Fcast", i)))
  
} 

rename((paste0("Fcast", 1), Fcst1 = FORECASTDEMAND)

##names(Fcst1)[1] <- "Fcst1"

paste0("Fcst", 1)
```


```{r}
# Make an empty list
mylist <- list()

# Populate it with each of the 32 different forecast models
for (i in 1:32){
  mylist[[i]] <- fcst_nsw2[fcst_nsw2$PERIODID == i, ]
}
```





