{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We would like to explore Random Forest Regressors and Support Vector Machines along with some input variables to see if we can make a model that accurately forecasts energy demand. We aim to achieve better results than simply saying 'The demand in 30 minutes time will be the same as it is right now'. This is calculated below as having a mean average loss of 218."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from warnings import filterwarnings\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "from pprint import pprint\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.svm import SVR\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Suppress annoying deprecation warnings\n",
    "filterwarnings(action='ignore', category=DeprecationWarning, module='sklearn')\n",
    "\n",
    "# Update working directory\n",
    "sys.path.append(os.path.abspath(os.getcwd()))\n",
    "\n",
    "# read data\n",
    "data = pd.read_csv('./Cleaned_Data.csv')\n",
    "\n",
    "# Set random state\n",
    "STATE = 20\n",
    "\n",
    "# Makes sure datetime is in datetime format\n",
    "data['DATETIME'] = pd.to_datetime(data['DATETIME'])\n",
    "\n",
    "# We want to test a time column as an input\n",
    "data['time'] = (data['DATETIME'].dt.strftime(\"%H%M%S\"))\n",
    "\n",
    "# Also would like to test demand and tmeperature 30 and 60 mins before the current time as input\n",
    "data['demand_30'] = data.TOTALDEMAND.shift(1)\n",
    "data['demand_60'] = data.TOTALDEMAND.shift(2)\n",
    "\n",
    "data['temp_30'] = data.TEMPERATURE.shift(1)\n",
    "data['temp_60'] = data.TEMPERATURE.shift(2)\n",
    "\n",
    "# Select data from 2016-2021\n",
    "mask = (data['DATETIME'] >= '2016-01-01') & (data['DATETIME'] < '2021-01-01')\n",
    "data = data.loc[mask]\n",
    "\n",
    "# And an indictor if it is a weekend or not\n",
    "data['is_weekday'] = data['DATETIME'].dt.weekday\n",
    "data['is_weekday'] = np.where(data['is_weekday'] < 5, 1, 0)\n",
    "\n",
    "# Create day column in data and merge using it\n",
    "data['date'] = data['DATETIME'].dt.date\n",
    "\n",
    "# Create day and month columns\n",
    "data['day'] = data['DATETIME'].dt.day\n",
    "data['month'] = data['DATETIME'].dt.month"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After creating a data frame with all of the features that we deem potentially interesting, we would like to evaluate their suitability for use in a random forest regressor. To do feature selection we will use mutual information (information gain of each input in relation to the output variable) as a selection metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.399191897637154 Feature:  time\n",
      "Score:  1.849544227146418 Feature:  demand_30\n",
      "Score:  1.2344058458589489 Feature:  demand_60\n",
      "Score:  0.1646852816195965 Feature:  temp_30\n",
      "Score:  0.16184459304100063 Feature:  temp_60\n",
      "Score:  0.03639470978405002 Feature:  is_weekday\n",
      "Score:  0.003227167929961361 Feature:  day\n",
      "Score:  0.121579263760355 Feature:  month\n"
     ]
    }
   ],
   "source": [
    "# Suppress annoying deprecation warnings\n",
    "filterwarnings(action='ignore', category=DeprecationWarning, module='sklearn')\n",
    "\n",
    "# Split into input and output dataframes\n",
    "Y = data['TOTALDEMAND']\n",
    "X = data.drop(columns = ['TEMPERATURE', 'TOTALDEMAND', 'DATETIME', 'date'])\n",
    "\n",
    "# Find the highest mutual information\n",
    "selector = SelectKBest(mutual_info_regression, k = 'all')\n",
    "X_train_new = selector.fit_transform(X, Y)  \n",
    "mask = selector.get_support()\n",
    "\n",
    "# Create a mask\n",
    "new_features = X.columns[mask]\n",
    "\n",
    "# Print features and their dependencies\n",
    "for i in range(len(new_features)):\n",
    "    print('Score: ', selector.scores_[i], 'Feature: ', new_features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From this, we can see that time, demand_30, demand_60, temp_30, temp_60, and month are all potentially useful in a model. temp_30 and temp_60 likely to be highly correlated and not useful to include both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create features and target sets\n",
    "features = data[['time', 'demand_30', 'demand_60', 'temp_30']]\n",
    "target = data['TOTALDEMAND']\n",
    "\n",
    "# Convert to numpy arrays and split training/test data\n",
    "features_np = pd.DataFrame(features).to_numpy()\n",
    "target_np = np.ravel(pd.DataFrame(target).to_numpy())\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_np,\n",
    "                                                                            target_np, random_state = STATE)\n",
    "\n",
    "# Implement Random Forest\n",
    "rnd_clf = RandomForestRegressor(random_state = STATE, criterion = 'absolute_error')\n",
    "rnd_clf.fit(features_train, target_train)\n",
    "\n",
    "# Print error\n",
    "rf_predicted = rnd_clf.predict(features_test)\n",
    "rf_MAE = mean_absolute_error(target_test, rf_predicted)\n",
    "print(\"Baseline Random Forest MAE: \", rf_MAE)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rnd_clf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have managed to eliminate most of the remaining inputs. We have; time, demand_30, demand_60 and temp_30 as useful to our model.\n",
    "\n",
    "Let's try random search of optimizing hyperparamters as a quick way to tell if our baseline model can perform any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Tune hyperparameters\n",
    "# Create the grid\n",
    "random_grid = {'n_estimators': list(range(80, 140, 20)),\n",
    "               'criterion': ['mae'],\n",
    "               'max_features': ['auto'],\n",
    "               'max_depth': list(range(10, 20, 5)),\n",
    "               'min_samples_split': list(range(2, 8, 2)),\n",
    "               'min_samples_leaf': list(range(1, 10, 2)),\n",
    "               'bootstrap': [True]}\n",
    "\n",
    "# Randomly saearch the grid for the best performance\n",
    "rf_random = RandomizedSearchCV(estimator = RandomForestRegressor(), param_distributions = random_grid, \n",
    "                               n_iter = 3, cv = 3, verbose = 2, random_state = STATE, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(features_train, target_train)\n",
    "\n",
    "# Print error\n",
    "rf_random_pred = rf_random.best_estimator_.predict(features_test)\n",
    "rf_random_MAE = mean_absolute_error(target_test, rf_random_pred)\n",
    "print(\"Baseline Random Forest MAE: \", rf_random_MAE)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This tuning is only a little better than our base model. No point in continuing hyperparpmeter tuning further, or in continuing with the model, unless we can think of other inputs. We will print some graphs to visualize model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "top_5_percent = []\n",
    "bottom_5_percent = []\n",
    "\n",
    "for x in range(len(features_test)):\n",
    "    estimator_predictions = []\n",
    "    for pred in rf_random.best_estimator_.estimators_:\n",
    "        estimator_predictions.append(pred.predict(features_test)[x])\n",
    "    top_5_percent.append(np.percentile(estimator_predictions, 95))\n",
    "    bottom_5_percent.append(np.percentile(estimator_predictions, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot predicted vs actual without error bars\n",
    "plt.scatter(target_test, rf_predicted)\n",
    "plt.plot()\n",
    "plt.xlabel('Actual Consumption')\n",
    "plt.ylabel('Predicted Consumption')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(target_test, rf_predicted, err_down, err_up)), columns = ['Actual_Values', 'Predicted_Values', \n",
    "                                                                                     'Lower_5%', 'Upper_5%'])\n",
    "df.to_csv(r'.\\conf_int_random_forest.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will now start with SVM modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Implement baseline Support Vector Machine Regressor\n",
    "regressor = SVR(kernel = 'rbf')\n",
    "regressor.fit(features_train, target_train)\n",
    "\n",
    "# Print error\n",
    "svm_predicted = regressor.predict(features_test)\n",
    "svm_MAE = mean_absolute_error(target_test, svm_predicted)\n",
    "print(\"SVM MAE: \", svm_MAE)\n",
    "\n",
    "# Look at parameters used\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(regressor.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The error from this SVM is a lot higher than that of the random forest. Hyperparameter tuning is unlikely to increase the model performance to a point where this model is useful. Below is code that can be used to perform tuning, it is commented out and suggested not to run as it takes so long."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Tune hyperparameters\n",
    "## Create the grid\n",
    "#SVM_param_grid = {'C' : [0.1, 1, 10],\n",
    "#                 'gamma' : [1, 0.1, 0.01],\n",
    "#                 'kernel' : ['rbf']}\n",
    "#\n",
    "## Make sure the model uses MAE as a score function\n",
    "#scorer = make_scorer(mean_absolute_error, greater_is_better = False)\n",
    "#\n",
    "## Randomly saearch the grid for the best performance\n",
    "#svr_gs = GridSearchCV(SVR(), SVM_param_grid)#, scoring=scorer)\n",
    "#svr_gs.fit(features_train, target_train)\n",
    "#\n",
    "## Create predicted values\n",
    "#grid_preds = svr_gs.predict(features_test)\n",
    "#\n",
    "#best_SVM_grid_error = mean_absolute_error(target_test, grid_preds)\n",
    "#\n",
    "## Print error\n",
    "#print(\"SVM Grid Search Tuning Error: \", best_SVM_grid_error)\n",
    "#\n",
    "## Look at parameters used\n",
    "#print('Parameters currently in use:\\n')\n",
    "#pprint(svr_gs.get_params())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}